# Tweet-Emotion-Detection
Fine-tuning transformer models using **PEFT** and **prompt-based learning** for multi-label sentiment classification of tweets.

---

## ðŸ“Œ Project Overview

This project explores the evolution of tweet emotion detection using various deep learning strategies, progressively optimizing for **accuracy**, **efficiency**, and **cost**.

### ðŸ§ª Stages of Development

- **HW5**: Custom MLP Model (Baseline)
- **HW6**: Full Fine-Tuning with Transformers (RoBERTa, ALBERT, DistilBERT)
- **HW7**: Parameter-Efficient Fine-Tuning (PEFT) using LoRA & BitsAndBytes
- **HW8**: Prompt-Based Fine-Tuning with PEFT (Meta LLaMA-3.2-1B-Instruct)

> ðŸš€ **Final Model**: `Meta LLaMA-3.2-1B-Instruct` with PEFT + Prompt-Based Tuning  
> ðŸŽ¯ **Accuracy**: 51.58% in detecting *optimism*

---

## ðŸ›  Tech Stack

- **Language**: Python  
- **Libraries**: `torch`, `transformers`, `peft`, `datasets`, `accelerate`, `wandb`  
- **Models**: RoBERTa, ALBERT, DistilBERT, LLaMA-3.2-1B, E5-Mistral-7B, Gemma  

---

## ðŸ”§ Installation

Ensure you have Python 3.8+ installed, then install required dependencies:

pip install torch transformers peft datasets accelerate wand

## ðŸ“‚ Project Structure

tweet-emotion-detection/
â”œâ”€â”€ HW5/                   # Custom MLP Model
â”œâ”€â”€ HW6/                   # Transformer Fine-Tuning
â”œâ”€â”€ HW7/                   # PEFT-Based Fine-Tuning
â”œâ”€â”€ HW8/                   # Prompt-Based Tuning with PEFT
â”œâ”€â”€ results/               # Submission Files & Evaluation Outputs
â”œâ”€â”€ models/                # Saved Fine-Tuned Models (optional)
â”œâ”€â”€ train.py               # Training Script
â”œâ”€â”€ evaluate.py            # Evaluation Script
â””â”€â”€ README.md              # Project Documentation
